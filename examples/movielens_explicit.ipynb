{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit feedback movie recommendations\n",
    "In this example, we'll build a quick explicit feedback recommender system: that is, a model that takes into account explicit feedback signals (like ratings) to recommend new content.\n",
    "\n",
    "We'll use an approach first made popular by the [Netflix prize](http://www.netflixprize.com/) contest: [matrix factorization](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf). \n",
    "\n",
    "<img src=\"static/matrix_factorization.png\" alt=\"Matrix factorization\" style=\"width: 600px;\"/>\n",
    "\n",
    "In matrix factorization, we start with user-item-rating triplets, conveying the information that user _i_ gave some item _j_ rating _r_. We then try to estimate representations for both users and items in some high-dimensional latent space so that when we multiply these representations, we can recover the original ratings. The utility of the model then is derived from the fact that if we multiply the user vector of a user with the item vector of some item they _have not_ rated, we hope to obtain a predicition for the rating they would have given to it if they had seen it.\n",
    "\n",
    "We start with importing a famous dataset, the [Movielens 100k dataset](https://grouplens.org/datasets/movielens/100k/). It contains 100,000 ratings (between 1 and 5) given to 1683 movies by 944 users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Interactions dataset (944 users x 1683 items x 100000 interactions)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "\n",
    "dataset = get_movielens_dataset(variant='100K')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the model, we'll split it into a train and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into \n",
      " <Interactions dataset (944 users x 1683 items x 80000 interactions)> and \n",
      " <Interactions dataset (944 users x 1683 items x 20000 interactions)>\n"
     ]
    }
   ],
   "source": [
    "random_state = np.random.RandomState(42)\n",
    "\n",
    "from spotlight.cross_validation import random_train_test_split\n",
    "\n",
    "train, test = random_train_test_split(dataset, random_state=random_state)\n",
    "\n",
    "print('Split into \\n {} and \\n {}'.format(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model structure\n",
    "\n",
    "We're going to fit a classic factorization model with a regression loss: that is, we'll be trying to fit latent representations to users and items in such a way that the squared difference between actual and predicted ratings is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
    "\n",
    "model = ExplicitFactorizationModel(loss='regression',\n",
    "                                  embedding_dim=128,\n",
    "                                  n_iter=10,\n",
    "                                  batch_size=1024,\n",
    "                                  l2=1e-9,\n",
    "                                  learning_rate=1e-3,\n",
    "                                  use_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent vectors (and biases) from factorization models are best represented by PyTorch embedding layers. This is done by the `BilinearNet` Spotlight class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Bilinear factorization representation.\n",
      "\n",
      "    Encodes both users and items as an embedding layer; the score\n",
      "    for a user-item pair is given by the dot product of the item\n",
      "    and user latent vectors.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    num_users: int\n",
      "        Number of users in the model.\n",
      "    num_items: int\n",
      "        Number of items in the model.\n",
      "    embedding_dim: int, optional\n",
      "        Dimensionality of the latent representations.\n",
      "    sparse: boolean, optional\n",
      "        Use sparse gradients.\n",
      "    \n",
      "    def __init__(self, num_users, num_items, embedding_dim=32, sparse=False):\n",
      "        super().__init__()\n",
      "\n",
      "        self.embedding_dim = embedding_dim\n",
      "\n",
      "        self.user_embeddings = ScaledEmbedding(num_users, embedding_dim,\n",
      "                                               sparse=sparse)\n",
      "        self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n",
      "                                               sparse=sparse)\n",
      "        self.user_biases = ZeroEmbedding(num_users, 1, sparse=sparse)\n",
      "        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "from spotlight.factorization.representations import BilinearNet\n",
    "\n",
    "print(BilinearNet.__doc__)\n",
    "print(inspect.getsource(BilinearNet.__init__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent vectors are represented by `ScaledEmbedding` layers (a straightforward extension of the base PyTorch embedding layer that modifies the intialization to scale down by the number of latent dimensions). Biases should be initialized with zeros, so they are represented by the `ZeroEmbedding` layers.\n",
    "\n",
    "The bulk of the work when fitting the model is done by the `forward` method, which accepts user and item indices, retrieves their embeddings and biases, takes their dot product, and returns the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(self, user_ids, item_ids):\n",
      "        \"\"\"\n",
      "        Compute the forward pass of the representation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "\n",
      "        user_ids: tensor\n",
      "            Tensor of user indices.\n",
      "        item_ids: tensor\n",
      "            Tensor of item indices.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "\n",
      "        predictions: tensor\n",
      "            Tensor of predictions.\n",
      "        \"\"\"\n",
      "\n",
      "        user_embedding = self.user_embeddings(user_ids)\n",
      "        item_embedding = self.item_embeddings(item_ids)\n",
      "\n",
      "        user_embedding = user_embedding.view(-1, self.embedding_dim)\n",
      "        item_embedding = item_embedding.view(-1, self.embedding_dim)\n",
      "\n",
      "        user_bias = self.user_biases(user_ids).view(-1, 1)\n",
      "        item_bias = self.item_biases(item_ids).view(-1, 1)\n",
      "\n",
      "        dot = (user_embedding * item_embedding).sum(1)\n",
      "\n",
      "        return dot + user_bias + item_bias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(BilinearNet.forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the predictions are returned, we can feed them (together with the observed ratings that we are trying to predict) into the loss function. In this case, it's a simple regression loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def regression_loss(observed_ratings, predicted_ratings):\n",
      "    \"\"\"\n",
      "    Regression loss.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    observed_ratings: tensor\n",
      "        Tensor containing observed ratings.\n",
      "    negative_predictions: tensor\n",
      "        Tensor containing rating predictions.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "\n",
      "    loss, float\n",
      "        The mean value of the loss function.\n",
      "    \"\"\"\n",
      "\n",
      "    assert_no_grad(observed_ratings)\n",
      "\n",
      "    return ((observed_ratings - predicted_ratings) ** 2).mean()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spotlight.losses import regression_loss\n",
    "\n",
    "print(inspect.getsource(regression_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 1034.392520904541\n",
      "Epoch 1: loss 569.925882101059\n",
      "Epoch 2: loss 136.80290246009827\n",
      "Epoch 3: loss 84.37578642368317\n",
      "Epoch 4: loss 74.30708056688309\n",
      "Epoch 5: loss 70.70860320329666\n",
      "Epoch 6: loss 68.6357769370079\n",
      "Epoch 7: loss 67.51402765512466\n",
      "Epoch 8: loss 66.53516966104507\n",
      "Epoch 9: loss 65.70937591791153\n"
     ]
    }
   ],
   "source": [
    "model.fit(train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE 0.897, test RMSE 0.940\n"
     ]
    }
   ],
   "source": [
    "from spotlight.evaluation import rmse_score\n",
    "\n",
    "train_rmse = rmse_score(model, train)\n",
    "test_rmse = rmse_score(model, test)\n",
    "\n",
    "print('Train RMSE {:.3f}, test RMSE {:.3f}'.format(train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
