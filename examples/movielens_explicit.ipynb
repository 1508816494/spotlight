{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit feedback movie recommendations\n",
    "In this example, we'll build a quick explicit feedback recommender system: that is, a model that takes into account explicit feedback signals (like ratings) to recommend new content.\n",
    "\n",
    "We'll use an approach first made popular by the [Netflix prize](http://www.netflixprize.com/) contest: [matrix factorization](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf). \n",
    "\n",
    "<img src=\"static/matrix_factorization.png\" alt=\"Matrix factorization\" style=\"width: 600px;\"/>\n",
    "\n",
    "In matrix factorization, we start with user-item-rating triplets, conveying the information that user _i_ gave some item _j_ rating _r_. We then try to estimate representations for both users and items in some high-dimensional latent space so that when we multiply these representations, we can recover the original ratings. The utility of the model then is derived from the fact that if we multiply the user vector of a user with the item vector of some item they _have not_ rated, we hope to obtain a predicition for the rating they would have given to it if they had seen it.\n",
    "\n",
    "We start with importing a famous dataset, the [Movielens 100k dataset](https://grouplens.org/datasets/movielens/100k/). It contains 100,000 ratings (between 1 and 5) given to 1683 movies by 944 users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Interactions dataset (944 users x 1683 items x 100000 interactions)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "\n",
    "dataset = get_movielens_dataset(variant='100K')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the model, we'll split it into a train and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into \n",
      " <Interactions dataset (944 users x 1683 items x 80000 interactions)> and \n",
      " <Interactions dataset (944 users x 1683 items x 20000 interactions)>\n"
     ]
    }
   ],
   "source": [
    "random_state = np.random.RandomState(42)\n",
    "\n",
    "from spotlight.cross_validation import random_train_test_split\n",
    "\n",
    "train, test = random_train_test_split(dataset, random_state=random_state)\n",
    "\n",
    "print('Split into \\n {} and \\n {}'.format(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to fit a classic factorization model with a regression loss: that is, we'll be trying to fit latent representations to users and items in such a way that the squared difference between actual and predicted ratings is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
    "\n",
    "model = ExplicitFactorizationModel(loss='regression',\n",
    "                                  embedding_dim=128,\n",
    "                                  n_iter=10,\n",
    "                                  batch_size=1024,\n",
    "                                  l2=1e-9,\n",
    "                                  learning_rate=1e-3,\n",
    "                                  use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 1034.392520904541\n",
      "Epoch 1: loss 569.925882101059\n",
      "Epoch 2: loss 136.80290246009827\n",
      "Epoch 3: loss 84.37578642368317\n",
      "Epoch 4: loss 74.30708056688309\n",
      "Epoch 5: loss 70.70860320329666\n",
      "Epoch 6: loss 68.6357769370079\n",
      "Epoch 7: loss 67.51402765512466\n",
      "Epoch 8: loss 66.53516966104507\n",
      "Epoch 9: loss 65.70937591791153\n"
     ]
    }
   ],
   "source": [
    "model.fit(train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE 0.897, test RMSE 0.940\n"
     ]
    }
   ],
   "source": [
    "from spotlight.evaluation import rmse_score\n",
    "\n",
    "train_rmse = rmse_score(model, train)\n",
    "test_rmse = rmse_score(model, test)\n",
    "\n",
    "print('Train RMSE {:.3f}, test RMSE {:.3f}'.format(train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
